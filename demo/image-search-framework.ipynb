{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install faiss-gpu\n",
    "!pip install gradio \n",
    "!pip install yolov5\n",
    "!pip install datasets==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from objectdetection import ObjectDetection\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from rerankForDemo import Initial, reranking\n",
    "import gradio as gr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using cache found in /home/jupyter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "init_shuffle = Initial(\"ShuffleNet\")\n",
    "init_vit = Initial(\"ViT\")\n",
    "init_efficient = Initial(\"EfficientNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open(\"data/img/CLOTHING/Blouse/id_00004848/comsumer_01.jpg\")\n",
    "img2 = Image.open(\"data/img/CLOTHING/Blouse/id_00004848/comsumer_01_blur.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32882434129714966]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranking(img1, [img2], init_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "od = ObjectDetection()\n",
    "#label, img, coor = od.detect(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bi_encoder import bi_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bi = bi_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration vietdata--fashion_database-efe547f9d25a9e94\n",
      "Found cached dataset parquet (/home/jupyter/.cache/huggingface/datasets/vietdata___parquet/vietdata--fashion_database-efe547f9d25a9e94/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e73157892e4486b46754c551d04e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408585dbcf124fecbf6a75ebe958ac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['embeddings', 'paths'],\n",
       "    num_rows: 22669\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import faiss\n",
    "\n",
    "bi.embeddings_dataset = load_dataset(\"vietdata/fashion_database\")[\"train\"]\n",
    "bi.embeddings_dataset.add_faiss_index(column=\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(img, mode=\"ShuffleNet\", label=None):\n",
    "    if mode == \"ShuffleNet\":\n",
    "        init = init_shuffle\n",
    "    elif mode == \"EfficientNet\":\n",
    "        init = init_efficient\n",
    "    else:\n",
    "        init = init_vit\n",
    "    related, scores = bi.search_images_from_image(img, topk=100, close=False, label=label)\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    #print(scores)\n",
    "    #sims = []\n",
    "    #for i in related:\n",
    "    #    sims.append(reranking(img, i, init))\n",
    "    sims = reranking(img, related, init, model=mode)\n",
    "    indices = np.argsort(np.array(sims))\n",
    "    #print(np.array(sims)[indices])\n",
    "    return [(related[i], sims[i]) for i in indices[::-1][:5]], [(x, float(y)) for x, y in zip(related[:5],scores[:5])]\n",
    "\n",
    "def object_retrieve(od_objects, mode, label: gr.SelectData):\n",
    "    labels, imgs, coors = od_objects\n",
    "    print(label.value)\n",
    "    for idx, label_ in enumerate(labels):\n",
    "        if label_ == label.value:\n",
    "            return retrieve(imgs[idx], mode, label=label.value)\n",
    "    return []\n",
    "\n",
    "def products(img):\n",
    "    labels, imgs, arr, coors = od.detect(img)\n",
    "    print(labels)\n",
    "    sections = [([int(i) for i in x], y) for x, y in zip(coors, labels)]\n",
    "    return (img, sections), (labels, imgs, coors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOPS_Summer_Suit']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x831>,\n",
       "  [([135, 150, 463, 587], 'TOPS_Summer_Suit')]),\n",
       " (['TOPS_Summer_Suit'],\n",
       "  [<PIL.Image.Image image mode=RGB size=318x328>],\n",
       "  [[tensor(135.53078, device='cuda:0'),\n",
       "    tensor(150.63867, device='cuda:0'),\n",
       "    tensor(463.15982, device='cuda:0'),\n",
       "    tensor(587.53485, device='cuda:0')]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products(Image.open(\"data/img/CLOTHING/Blouse/id_00004848/comsumer_01.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.866838812828064),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.8544437289237976),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.8481858968734741),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.8459996581077576),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.8444223999977112)],\n",
       " [(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.08092286895089286),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.11095014810134765),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.2149888355058659),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.11430344676379736),\n",
       "  (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=468x702>,\n",
       "   0.3537466726033517)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve(Image.open(\"data/img/CLOTHING/Blouse/id_00004848/comsumer_01.jpg\"), label=\"TOPS_Coat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://bd48da0a3251fef6b2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bd48da0a3251fef6b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOPS_Coat']\n",
      "TOPS_Coat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    od_objects = gr.State(None)\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # <center> Image Search Framework </center>\n",
    "            \"\"\")\n",
    "        with gr.Row(equal_height=False):\n",
    "            with gr.Column():\n",
    "                image = gr.Image(type=\"pil\")\n",
    "                analyze = gr.Button(\"Analyze\")\n",
    "                search = gr.Button(\"Search Full Image\")\n",
    "                mode = gr.Radio([\"ShuffleNet\", \"EfficientNet\", \"ViT\"], value=\"ShuffleNet\")\n",
    "\n",
    "            with gr.Column():\n",
    "                with gr.Row():\n",
    "                    img_output = gr.AnnotatedImage()\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    # <center> Search By Each Product </center>\n",
    "                    \"\"\")\n",
    "                \n",
    "        with gr.Row(equal_height=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                # <center> Before Reranking </center>\n",
    "                \"\"\")\n",
    "            gallery11 = gr.Gallery(\n",
    "            label=\"Search Results\", show_label=False, elem_id=\"gallery\"\n",
    "            ).style(columns=[2], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "                # <center> Before reranking </center>\n",
    "                \"\"\")\n",
    "            gallery21 = gr.Gallery(\n",
    "            label=\"Search Results\", show_label=False, elem_id=\"gallery\"\n",
    "            ).style(columns=[2], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "\n",
    "\n",
    "                \n",
    "        with gr.Row():\n",
    "            gr.Markdown(\n",
    "            \"\"\"\n",
    "            # <center> After Reranking </center>\n",
    "            \"\"\")\n",
    "            gallery12 = gr.Gallery(\n",
    "                label=\"Search Results\", show_label=False, elem_id=\"gallery\"\n",
    "                ).style(columns=[2], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "\n",
    "            gr.Markdown(\"\"\"\n",
    "            # <center> After reranking </center>\n",
    "            \"\"\")\n",
    "            gallery22 = gr.Gallery(\n",
    "                label=\"Search Results\", show_label=False, elem_id=\"gallery\"\n",
    "                ).style(columns=[2], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "        search.click(fn=retrieve,\n",
    "                    inputs=[image, mode],\n",
    "                    outputs=[gallery12, gallery11])\n",
    "        analyze.click(products, [image], [img_output, od_objects])\n",
    "        img_output.select(object_retrieve, [od_objects, mode], [gallery22, gallery21])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
