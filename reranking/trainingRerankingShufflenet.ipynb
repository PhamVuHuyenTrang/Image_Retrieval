{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"import random\nimport os\nimport itertools\nfrom tqdm import tqdm\n\nrandom.seed(0)\n\npartition_dir = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/Eval/list_eval_partition.txt\"\npartition = open(partition_dir, \"r\")\nlst = []\nwith open(partition_dir, 'r') as f:\n    for i,line in enumerate(f):\n        if i < 2:\n            continue\n        lst.append(line.split())\n\nrandom.shuffle(lst)\n\nstatistic = {'train': 0, 'val': 0, 'test': 0}\ndata = {'train': [], 'val': [], 'test': []}\nmax_statistic = {'train': 8000, 'val': 1000, 'test': 1000}\ntrain, valid, test = [], [], []\nfor sample in tqdm(lst):\n    if statistic == max_statistic:\n        break\n    sample[0] = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/\" + sample[0]\n    sample[1] = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/\" + sample[1]\n    if not (os.path.exists(sample[0]) and os.path.exists(sample[1])):\n        continue\n    type_sample = sample[3]\n    sample[2] = 1\n    if statistic[type_sample] < max_statistic[type_sample]:\n        statistic[type_sample] += 1\n        data[type_sample].append(sample)\n        \nimg_dir = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/img\"\nclothesDir = [img_dir + \"/\" + i for i in os.listdir(img_dir)]\ntypeDir = list(itertools.chain.from_iterable([[dir + \"/\" + i for i in os.listdir(dir)] for dir in clothesDir]))\nidDir = list(itertools.chain.from_iterable([[dir + \"/\" + i for i in os.listdir(dir)] for dir in typeDir]))\n\nstatistic = {'train': 0, 'val': 0, 'test': 0}\nrandom.shuffle(idDir)\nidDir = idDir[:1000]\npair = []\nfor i in range(len(idDir)):\n    for j in range(i+1, len(idDir)):\n        pair.append((i, j))\nrandom.shuffle(pair)\npair = pair[:12000]\nstatus = \"train\"\nfor i, sample in tqdm(enumerate(pair)):\n    id1 = idDir[sample[0]]\n    id2 = idDir[sample[1]]\n    comsumer = sorted(os.listdir(id1))[0]\n    shop = sorted(os.listdir(id2))[-1]\n    if \"comsumer\" in comsumer and \"shop\" in shop:\n        comsumer_dir = id1+\"/\"+comsumer\n        shop_dir = id2+\"/\"+shop\n    else:\n        continue\n    data[status].append([comsumer_dir, shop_dir, 0, status])\n    statistic[status] += 1\n    if statistic[status] >= max_statistic[status]:\n        if status == 'train':\n            status = 'val'\n        elif status == 'val':\n            status = 'test'\n        else:\n            break\n\nrandom.shuffle(data['train'])\nrandom.shuffle(data['val'])\nrandom.shuffle(data['test'])\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:17:26.676544Z","iopub.execute_input":"2023-07-08T13:17:26.676887Z","iopub.status.idle":"2023-07-08T13:17:26.693241Z","shell.execute_reply.started":"2023-07-08T13:17:26.676853Z","shell.execute_reply":"2023-07-08T13:17:26.692229Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'import random\\nimport os\\nimport itertools\\nfrom tqdm import tqdm\\n\\nrandom.seed(0)\\n\\npartition_dir = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/Eval/list_eval_partition.txt\"\\npartition = open(partition_dir, \"r\")\\nlst = []\\nwith open(partition_dir, \\'r\\') as f:\\n    for i,line in enumerate(f):\\n        if i < 2:\\n            continue\\n        lst.append(line.split())\\n\\nrandom.shuffle(lst)\\n\\nstatistic = {\\'train\\': 0, \\'val\\': 0, \\'test\\': 0}\\ndata = {\\'train\\': [], \\'val\\': [], \\'test\\': []}\\nmax_statistic = {\\'train\\': 8000, \\'val\\': 1000, \\'test\\': 1000}\\ntrain, valid, test = [], [], []\\nfor sample in tqdm(lst):\\n    if statistic == max_statistic:\\n        break\\n    sample[0] = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/\" + sample[0]\\n    sample[1] = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/\" + sample[1]\\n    if not (os.path.exists(sample[0]) and os.path.exists(sample[1])):\\n        continue\\n    type_sample = sample[3]\\n    sample[2] = 1\\n    if statistic[type_sample] < max_statistic[type_sample]:\\n        statistic[type_sample] += 1\\n        data[type_sample].append(sample)\\n        \\nimg_dir = \"/kaggle/input/customer2shopdataset/Customer2Shop/Customer2Shop/img\"\\nclothesDir = [img_dir + \"/\" + i for i in os.listdir(img_dir)]\\ntypeDir = list(itertools.chain.from_iterable([[dir + \"/\" + i for i in os.listdir(dir)] for dir in clothesDir]))\\nidDir = list(itertools.chain.from_iterable([[dir + \"/\" + i for i in os.listdir(dir)] for dir in typeDir]))\\n\\nstatistic = {\\'train\\': 0, \\'val\\': 0, \\'test\\': 0}\\nrandom.shuffle(idDir)\\nidDir = idDir[:1000]\\npair = []\\nfor i in range(len(idDir)):\\n    for j in range(i+1, len(idDir)):\\n        pair.append((i, j))\\nrandom.shuffle(pair)\\npair = pair[:12000]\\nstatus = \"train\"\\nfor i, sample in tqdm(enumerate(pair)):\\n    id1 = idDir[sample[0]]\\n    id2 = idDir[sample[1]]\\n    comsumer = sorted(os.listdir(id1))[0]\\n    shop = sorted(os.listdir(id2))[-1]\\n    if \"comsumer\" in comsumer and \"shop\" in shop:\\n        comsumer_dir = id1+\"/\"+comsumer\\n        shop_dir = id2+\"/\"+shop\\n    else:\\n        continue\\n    data[status].append([comsumer_dir, shop_dir, 0, status])\\n    statistic[status] += 1\\n    if statistic[status] >= max_statistic[status]:\\n        if status == \\'train\\':\\n            status = \\'val\\'\\n        elif status == \\'val\\':\\n            status = \\'test\\'\\n        else:\\n            break\\n\\nrandom.shuffle(data[\\'train\\'])\\nrandom.shuffle(data[\\'val\\'])\\nrandom.shuffle(data[\\'test\\'])'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"import os\nos.mkdir(\"/kaggle/working/owndataset/\")\nwith open(r'/kaggle/working/owndataset/train.txt', 'w') as fp:\n    for item in data['train']:\n        fp.write(\"%s\\n\" % item)\n    print('Done')\nwith open(r'/kaggle/working/owndataset/val.txt', 'w') as fp:\n    for item in data['val']:\n        fp.write(\"%s\\n\" % item)\n    print('Done')\nwith open(r'/kaggle/working/owndataset/test.txt', 'w') as fp:\n    for item in data['test']:\n        fp.write(\"%s\\n\" % item)\n    print('Done')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:17:26.694703Z","iopub.execute_input":"2023-07-08T13:17:26.695032Z","iopub.status.idle":"2023-07-08T13:17:26.707472Z","shell.execute_reply.started":"2023-07-08T13:17:26.695003Z","shell.execute_reply":"2023-07-08T13:17:26.706514Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'import os\\nos.mkdir(\"/kaggle/working/owndataset/\")\\nwith open(r\\'/kaggle/working/owndataset/train.txt\\', \\'w\\') as fp:\\n    for item in data[\\'train\\']:\\n        fp.write(\"%s\\n\" % item)\\n    print(\\'Done\\')\\nwith open(r\\'/kaggle/working/owndataset/val.txt\\', \\'w\\') as fp:\\n    for item in data[\\'val\\']:\\n        fp.write(\"%s\\n\" % item)\\n    print(\\'Done\\')\\nwith open(r\\'/kaggle/working/owndataset/test.txt\\', \\'w\\') as fp:\\n    for item in data[\\'test\\']:\\n        fp.write(\"%s\\n\" % item)\\n    print(\\'Done\\')'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"from ast import literal_eval\nread_data = {}\nreads = []\nwith open(r'/kaggle/working/owndataset/train.txt', 'r') as fp:\n    for line in fp:\n        reads.append(line)\n    print(\"Done\")\nread_data['train'] = [literal_eval(line[:-1]) for line in reads]\nreads = []\nwith open(r'/kaggle/working/owndataset/val.txt', 'r') as fp:\n    for line in fp:\n        reads.append(line)\n    print(\"Done\")\nread_data['val'] = [literal_eval(line[:-1]) for line in reads]\nreads = []\nwith open(r'/kaggle/working/owndataset/test.txt', 'r') as fp:\n    for line in fp:\n        reads.append(line)\n    print(\"Done\")\nread_data['test'] = [literal_eval(line[:-1]) for line in reads]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:17:26.708634Z","iopub.execute_input":"2023-07-08T13:17:26.709484Z","iopub.status.idle":"2023-07-08T13:17:26.722957Z","shell.execute_reply.started":"2023-07-08T13:17:26.709452Z","shell.execute_reply":"2023-07-08T13:17:26.722066Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'from ast import literal_eval\\nread_data = {}\\nreads = []\\nwith open(r\\'/kaggle/working/owndataset/train.txt\\', \\'r\\') as fp:\\n    for line in fp:\\n        reads.append(line)\\n    print(\"Done\")\\nread_data[\\'train\\'] = [literal_eval(line[:-1]) for line in reads]\\nreads = []\\nwith open(r\\'/kaggle/working/owndataset/val.txt\\', \\'r\\') as fp:\\n    for line in fp:\\n        reads.append(line)\\n    print(\"Done\")\\nread_data[\\'val\\'] = [literal_eval(line[:-1]) for line in reads]\\nreads = []\\nwith open(r\\'/kaggle/working/owndataset/test.txt\\', \\'r\\') as fp:\\n    for line in fp:\\n        reads.append(line)\\n    print(\"Done\")\\nread_data[\\'test\\'] = [literal_eval(line[:-1]) for line in reads]'"},"metadata":{}}]},{"cell_type":"code","source":"# read_data['train'][:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:17:26.727576Z","iopub.execute_input":"2023-07-08T13:17:26.727862Z","iopub.status.idle":"2023-07-08T13:17:26.732384Z","shell.execute_reply.started":"2023-07-08T13:17:26.727839Z","shell.execute_reply":"2023-07-08T13:17:26.731530Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nread_data = {}\ndata = pd.read_csv(\"/kaggle/input/add-csv/pairs_for_training.csv\")\ndata = shuffle(data)\nprefix = \"/kaggle/input/imageretrievaldataset/\"\na = [data.loc[i].tolist()[1:5] for i in tqdm(range(len(data)))]\nb = []\nk = 0\ni = 0\npbar = tqdm(total=len(data)+1)\nwhile i < len(data):\n    if os.path.exists(prefix+a[i][0]) and os.path.exists(prefix+a[i][1]):\n        b.append([prefix+a[i][0], prefix+a[i][1], a[i][3], a[i][2]])\n        k+=1\n        pbar.update(1)\n    i += 1\npbar.close()\nprint(\"Available: \", k)\nread_data['train'] = b","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:17:26.733870Z","iopub.execute_input":"2023-07-08T13:17:26.734605Z","iopub.status.idle":"2023-07-08T13:20:04.479158Z","shell.execute_reply.started":"2023-07-08T13:17:26.734574Z","shell.execute_reply":"2023-07-08T13:20:04.477644Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 268608/268608 [00:29<00:00, 9250.88it/s]\n100%|█████████▉| 268608/268609 [02:07<00:00, 2105.63it/s]","output_type":"stream"},{"name":"stdout","text":"Available:  268608\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/imageretrievaldataset/pairs_for_testing.csv\")\ndata = shuffle(data)\nprefix = \"/kaggle/input/imageretrievaldataset/\"\na = [data.loc[i].tolist()[1:6] for i in tqdm(range(len(data)))]\nb = []\nk = 0\ni = 0\npbar = tqdm(total=len(data)+1)\nwhile i < len(data):\n    if os.path.exists(prefix+a[i][0]) and os.path.exists(prefix+a[i][1]):\n        b.append([prefix+a[i][0], prefix+a[i][1], a[i][4], a[i][3]])\n        k+=1\n        pbar.update(1)\n    i += 1\npbar.close()\nprint(\"Available: \", k)\nread_data['test'] = b","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:21:38.290249Z","iopub.execute_input":"2023-07-08T13:21:38.290638Z","iopub.status.idle":"2023-07-08T13:22:43.546315Z","shell.execute_reply.started":"2023-07-08T13:21:38.290605Z","shell.execute_reply":"2023-07-08T13:22:43.545353Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 96708/96708 [00:09<00:00, 10038.13it/s]\n100%|█████████▉| 96708/96709 [00:55<00:00, 1746.77it/s]","output_type":"stream"},{"name":"stdout","text":"Available:  96708\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"a = []\nfor i in read_data['train']:\n    if type(i[2]) not in a:\n        a.append(type(i[2]))\n        print(i[2])\n\nprint(a)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:22:43.548686Z","iopub.execute_input":"2023-07-08T13:22:43.549349Z","iopub.status.idle":"2023-07-08T13:22:43.616936Z","shell.execute_reply.started":"2023-07-08T13:22:43.549312Z","shell.execute_reply":"2023-07-08T13:22:43.616022Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0.75\n[<class 'numpy.float64'>]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass huydataset(Dataset):\n    def __init__(self, file):\n        firsts = []\n        seconds = []\n        labels = []\n        for line in file:\n            path1, path2, label, _ = line\n            firsts.append(path1)\n            seconds.append(path2)\n            labels.append(float(label))\n        self.data = {\"firsts\":firsts, \"seconds\":seconds, \"labels\":labels}\n        self.preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n    \n    def __len__(self):\n        return len(self.data[\"labels\"])\n    \n    def __getitem__(self, idx):\n        def merge_width(image1, image2):\n            new_image = Image.new(\"RGB\", (image1.size[0], image1.size[1]+image2.size[1]), (250, 250, 250))\n            new_image.paste(image1,(0,0))\n            new_image.paste(image2,(0,image1.size[1]))\n            return new_image\n        \n        def merge_height(image1, image2):\n            new_image = Image.new(\"RGB\", (256, 256), (250, 250, 250))\n            new_image.paste(image1,(0,0))\n            new_image.paste(image2,(128,0))\n            return new_image\n        \n        img1 = Image.open(self.data[\"firsts\"][idx])\n        img1 = transforms.Resize((256, 128))(img1)\n        img2 = Image.open(self.data[\"seconds\"][idx])\n        img2 = transforms.Resize((256, 128))(img2)\n        merged_imgs = self.preprocess(merge_height(img1, img2)).unsqueeze(0)\n        return {\"labels\":self.data[\"labels\"][idx], \"merged_imgs\":merged_imgs}\n        \n    def collate_fn(self, batch):\n        def get_data(batch):\n            data = {i:[] for i in batch[0]}\n            for i in batch:\n                for k in i:\n                    data[k].append(i[k])\n            return data\n        \n        batch = get_data(batch)\n        batch[\"merged_imgs\"] = torch.cat(batch[\"merged_imgs\"], dim=0)\n        batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:22:43.618343Z","iopub.execute_input":"2023-07-08T13:22:43.618712Z","iopub.status.idle":"2023-07-08T13:22:45.089548Z","shell.execute_reply.started":"2023-07-08T13:22:43.618664Z","shell.execute_reply":"2023-07-08T13:22:45.088615Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass rerankingShuffleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)\n        self.model.fc = nn.Linear(1024, 1)\n        self.model.train()\n        self.act = nn.Sigmoid()\n        self.crit = nn.MSELoss()\n    \n    def forward(self, labels, merged_imgs):\n        logits = self.model(merged_imgs)\n        output = self.act(logits)\n        loss = self.crit(output, labels.unsqueeze(1))\n        return loss, output","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:22:45.092626Z","iopub.execute_input":"2023-07-08T13:22:45.093309Z","iopub.status.idle":"2023-07-08T13:22:45.100195Z","shell.execute_reply.started":"2023-07-08T13:22:45.093263Z","shell.execute_reply":"2023-07-08T13:22:45.099113Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import ViTImageProcessor, ViTForImageClassification, ViTModel\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torchvision import transforms\n\n\ntrain_dataset = huydataset(read_data['train'])\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=train_dataset.collate_fn)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = rerankingShuffleNet().to(device)\noptimizer = torch.optim.AdamW(params  = model.parameters(), lr = 1e-3)\nlosses = []\navg_losses = []\nfor epoch in range(5):\n    train_iters = tqdm(train_loader)\n    avg_loss = 0\n    losse = []\n    avg_losse = []\n    for idx, batch in enumerate(train_iters):\n        for key in ['labels', 'merged_imgs']:\n            batch[key] = batch[key].to(device)\n        loss, output = model(**batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        avg_loss = (avg_loss*idx+loss)/(idx+1)\n        train_iters.set_postfix({\"avg_loss\":avg_loss.item(), 'loss':loss.item()})\n        losse.append(loss.item())\n        avg_losse.append(avg_loss.item())\n    losses.append(losse)\n    avg_losses.append(avg_losse)\n    \n    torch.save(model.state_dict(), \"/kaggle/working/model.bin\")\n    from huggingface_hub import HfApi\n    api = HfApi()\n    api.upload_file(\n        path_or_fileobj=\"/kaggle/working/model.bin\",\n        path_in_repo=\"model.bin\",\n        repo_id=\"Huy1432884/rerankingShuffleNet\",\n        repo_type=\"model\",\n        token=\"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\"\n    )\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:22:45.102751Z","iopub.execute_input":"2023-07-08T13:22:45.103301Z","iopub.status.idle":"2023-07-08T13:26:22.047119Z","shell.execute_reply.started":"2023-07-08T13:22:45.103259Z","shell.execute_reply":"2023-07-08T13:26:22.044563Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nUsing cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n  5%|▌         | 454/8394 [03:29<1:00:56,  2.17it/s, avg_loss=0.0909, loss=0.166] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m losse \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m avg_losse \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iters):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_imgs\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     24\u001b[0m         batch[key] \u001b[38;5;241m=\u001b[39m batch[key]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mhuydataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_image\n\u001b[1;32m     37\u001b[0m img1 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirsts\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx])\n\u001b[0;32m---> 38\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m img2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx])\n\u001b[1;32m     40\u001b[0m img2 \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m))(img2)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2157\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2155\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[0;32m-> 2157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2159\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"avg_loss: 0.1523","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.plot(losses[4])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.048547Z","iopub.status.idle":"2023-07-08T13:26:22.049096Z","shell.execute_reply.started":"2023-07-08T13:26:22.048841Z","shell.execute_reply":"2023-07-08T13:26:22.048865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(avg_losses[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.051401Z","iopub.status.idle":"2023-07-08T13:26:22.052317Z","shell.execute_reply.started":"2023-07-08T13:26:22.052048Z","shell.execute_reply":"2023-07-08T13:26:22.052073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/model.bin\")\nfrom huggingface_hub import HfApi\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/model.bin\",\n    path_in_repo=\"model.bin\",\n    repo_id=\"Huy1432884/rerankingShuffleNet\",\n    repo_type=\"model\",\n    token=\"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\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.053946Z","iopub.status.idle":"2023-07-08T13:26:22.054800Z","shell.execute_reply.started":"2023-07-08T13:26:22.054505Z","shell.execute_reply":"2023-07-08T13:26:22.054529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile reranking.py\n\nfrom huggingface_hub import hf_hub_download\nimport torch.nn as nn\nfrom PIL import Image\nfrom torchvision import transforms\nfrom transformers import ViTImageProcessor, ViTForImageClassification, ViTModel\nimport torchvision.transforms as T\nimport torch\n\nclass Initial:\n    def __init__(self):\n        self.process = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        \n        self.model = rerankingShuffleNet()\n        version = None\n        # version = \"8c9e868d2c0287d60b23df6d6f29607625356a7d\"\n        hf_hub_download(repo_id=\"Huy1432884/rerankingShuffleNet\", \n            filename=\"model.bin\", \n            use_auth_token=\"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\", \n            local_dir=\"/kaggle/working/\",\n            revision=version\n        )\n        self.model.load_state_dict(torch.load(\"/kaggle/working/model.bin\", map_location=torch.device('cpu')))\n        self.model.eval()\n        \n        \ndef reranking(image1, image2, initial):\n    def merge_height(image1, image2):\n        new_image = Image.new(\"RGB\", (224, 224), (250, 250, 250))\n        new_image.paste(image1,(0,0))\n        new_image.paste(image2,(112,0))\n        return new_image\n    \n    img1 = transforms.Resize((224, 112))(image1)\n    img2 = transforms.Resize((224, 112))(image2)\n    merged_imgs = merge_height(img1, img2)\n    merged_imgs = initial.process(merged_imgs).unsqueeze(0)\n    output = initial.model(merged_imgs)\n    output = output.squeeze(0).squeeze(0).item()\n    return output\n    \nclass rerankingShuffleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)\n        self.model.fc = nn.Linear(1024, 1)\n        self.model.eval()\n        self.act = nn.Sigmoid()\n        self.crit = nn.MSELoss()\n    \n    def forward(self, merged_imgs):\n        logits = self.model(merged_imgs)\n        output = self.act(logits)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.056508Z","iopub.status.idle":"2023-07-08T13:26:22.057141Z","shell.execute_reply.started":"2023-07-08T13:26:22.056901Z","shell.execute_reply":"2023-07-08T13:26:22.056925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial = Initial()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.058793Z","iopub.status.idle":"2023-07-08T13:26:22.059829Z","shell.execute_reply.started":"2023-07-08T13:26:22.059568Z","shell.execute_reply":"2023-07-08T13:26:22.059591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.063482Z","iopub.status.idle":"2023-07-08T13:26:22.064418Z","shell.execute_reply.started":"2023-07-08T13:26:22.064173Z","shell.execute_reply":"2023-07-08T13:26:22.064197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img1)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.068000Z","iopub.status.idle":"2023-07-08T13:26:22.068821Z","shell.execute_reply.started":"2023-07-08T13:26:22.068525Z","shell.execute_reply":"2023-07-08T13:26:22.068549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img2)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.070396Z","iopub.status.idle":"2023-07-08T13:26:22.071172Z","shell.execute_reply.started":"2023-07-08T13:26:22.070923Z","shell.execute_reply":"2023-07-08T13:26:22.070946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor i, sample in enumerate(read_data['train']):\n    label = sample[2]\n    if label != 1:\n        continue\n    image1_dir = sample[0]\n    image2_dir = sample[1]\n    if i == 12:\n        img1 = Image.open(image1_dir)\n        img2 = Image.open(image2_dir)\n    image1 = Image.open(image1_dir)\n    image2 = Image.open(image2_dir)\n    output = reranking(image1, image2, initial)\n    print(label)\n    print(output)\n    print('-'*100)\n    if i > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.072592Z","iopub.status.idle":"2023-07-08T13:26:22.073356Z","shell.execute_reply.started":"2023-07-08T13:26:22.073106Z","shell.execute_reply":"2023-07-08T13:26:22.073130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, sample in enumerate(read_data['test']):\n    image1_dir = sample[0]\n    image2_dir = sample[1]\n    label = sample[2]\n    image1 = Image.open(image1_dir)\n    image2 = Image.open(image2_dir)\n    output = reranking(image1, image2, initial)\n    print(label)\n    print(output)\n    print('-'*100)\n    if i > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-07-08T13:26:22.074701Z","iopub.status.idle":"2023-07-08T13:26:22.075451Z","shell.execute_reply.started":"2023-07-08T13:26:22.075211Z","shell.execute_reply":"2023-07-08T13:26:22.075234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}